---
title: "Two-Stage Retrieval for AI Agents"
description: "Strategic approach to codebase retrieval using git primitives, embeddings, and reranking for optimal AI agent performance."
---

## Why Two-Stage Retrieval?

When building AI agents that work with large codebases, naive approaches quickly break down. Simply feeding entire repositories into context windows leads to:

- **Context pollution**: Irrelevant files confuse the model and degrade output quality
- **Token explosion**: Large context windows become prohibitively expensive
- **Performance degradation**: Processing time increases exponentially with context size
- **Accuracy loss**: Important signals get lost in noise

Two-stage retrieval solves these problems by intelligently filtering codebases before expensive processing.

## The Two-Stage Approach

### Stage 1: Broad Retrieval (Cast a Wide Net)

Quickly identify potentially relevant files using fast, lightweight methods:

- **Git primitives**: Leverage git's built-in indexing for file discovery
- **Keyword matching**: Find files containing specific terms or patterns
- **Semantic embeddings**: Use vector similarity for conceptual matches
- **Recency signals**: Weight recently changed files higher

**Goal**: Reduce 10,000+ files to ~50-100 candidates in less than 2 seconds

### Stage 2: Precise Ranking (Narrow to Essentials)

Apply sophisticated ranking to find the most relevant subset:

- **Code Reranker**: Relace's specialized model for code relevance
- **Context-aware scoring**: Understand relationships between query and code
- **Token-aware filtering**: Respect context window limits

**Goal**: Reduce candidates to 5-15 highly relevant files for final context

## Performance Benchmarks

### Speed Comparison

| Approach                | Time to Process 5,000 Files | Accuracy (Recall@10) |
| :---------------------- | :-------------------------- | :------------------- |
| **Naive (all files)**   | 45-60 seconds               | 95% (but unusable)   |
| **Git grep only**       | 0.5 seconds                 | 65%                  |
| **Embeddings only**     | 8-12 seconds                | 82%                  |
| **Two-stage retrieval** | 3-5 seconds                 | 91%                  |

### Token Efficiency

Two-stage retrieval dramatically reduces token usage while maintaining quality:

- **Before**: 150,000+ tokens per request (entire relevant codebase)
- **After**: 8,000-15,000 tokens per request (filtered files only)
- **Savings**: 85-90% reduction in input tokens
- **Quality**: Minimal degradation in output accuracy

### Cost Impact

For a typical AI agent making 100 requests per day:

```
Traditional approach: 150K tokens × 100 requests × $0.003/1K = $45/day
Two-stage approach: 12K tokens × 100 requests × $0.003/1K = $3.60/day

Monthly savings: ~$1,200
```

## Strategic Benefits

### 1. Scale Independence

Two-stage retrieval performance scales logarithmically, not linearly:

- **Small codebases** (100-500 files): 1-2 second retrieval
- **Medium codebases** (1,000-5,000 files): 2-4 second retrieval
- **Large codebases** (10,000+ files): 3-6 second retrieval

### 2. Quality Preservation

Unlike naive filtering, two-stage retrieval maintains high accuracy:

- **Semantic understanding**: Captures conceptual relationships beyond keywords
- **Context awareness**: Understands code dependencies and patterns
- **Multi-signal fusion**: Combines multiple relevance indicators

### 3. Adaptability

The approach works across different scenarios:

- **Bug fixes**: Finds related error-handling code and test files
- **Feature additions**: Identifies similar patterns and integration points
- **Refactoring**: Locates all usage sites and dependencies
- **Documentation**: Connects code to relevant docs and examples

## Stage 1 Strategies: Broad Retrieval

### Git Primitives for Fast Discovery

Git's built-in indexing provides several powerful retrieval methods:

#### File System Navigation

- **`git ls-files`**: Get all tracked files with extension filtering
- **`git diff --name-only`**: Find recently changed files
- **`git grep -l`**: Locate files containing specific patterns
- **Performance**: Sub-second for repositories with 10,000+ files

#### Recency Signals

Recent changes often indicate ongoing work areas:

- Files modified in last 1-5 commits get relevance boost
- Active branches may contain related work
- Useful for iterative development workflows

#### Pattern Matching

Fast keyword-based filtering:

- Function names, class names, variable names
- Import statements and dependencies
- Error messages and logging patterns
- Comments and documentation keywords

### Semantic Embeddings for Conceptual Matches

Vector similarity captures relationships that keywords miss:

#### What Embeddings Excel At

- **Conceptual similarity**: "authentication" matches "login", "security", "tokens"
- **Cross-language patterns**: Similar logic in different programming languages
- **Architectural relationships**: Components that work together
- **Intent matching**: User requests to implementation patterns

#### Embedding Strategy

1. **Pre-compute embeddings** for all relevant source files
2. **Cache embeddings** tied to git commit hashes
3. **Incremental updates** for changed files only
4. **Batch processing** for new repositories

### Multi-Signal Fusion

Combine multiple retrieval methods for comprehensive coverage:

| Signal Type    | Speed   | Precision | Coverage | Best For              |
| :------------- | :------ | :-------- | :------- | :-------------------- |
| **Git grep**   | Fastest | High      | Limited  | Exact matches         |
| **File paths** | Fast    | Medium    | Medium   | Structural navigation |
| **Recency**    | Fast    | Low       | Narrow   | Active development    |
| **Embeddings** | Medium  | High      | Broad    | Conceptual matches    |

**Optimal Strategy**: Use all signals in parallel, weight by query type

## Stage 2 Strategies: Precise Ranking

### Why Code Reranker is Essential

Stage 1 gives you candidates, but Stage 2 determines quality. Relace's Code Reranker provides:

#### Superior Accuracy

- **99%+ recall** at conservative thresholds
- **91% recall** with aggressive token savings
- **2x faster** than frontier model alternatives
- **2/3 the cost** of Gemini Flash-Lite approaches

#### Context-Aware Understanding

The reranker was trained specifically for code relevance tasks:

- **Understands code relationships**: Functions, classes, imports, dependencies
- **Semantic code patterns**: Recognizes similar implementations across languages
- **Intent matching**: Maps user requests to relevant code sections
- **Architectural awareness**: Identifies related components and layers

### Threshold Strategy Selection

Choose your relevance threshold based on your constraints:

#### Conservative (Threshold ≤ 0.02)

- **Best for**: Critical applications where missing relevant code is costly
- **Token savings**: ~25% reduction
- **Recall**: 99%+
- **Use case**: Production bug fixes, security implementations

#### Balanced (Threshold ≤ 0.08)

- **Best for**: General development tasks
- **Token savings**: ~45% reduction
- **Recall**: 95%+
- **Use case**: Feature development, refactoring, documentation

#### Aggressive (Threshold 0.15-0.25)

- **Best for**: Exploratory work, prototyping
- **Token savings**: Up to 70% reduction
- **Recall**: 80-90%
- **Use case**: Initial research, proof of concepts

## Implementation Patterns

### Workflow Architecture

A typical two-stage retrieval workflow follows this pattern:

#### Initialization Phase

1. **Index the codebase** once using git primitives and embeddings
2. **Cache embeddings** tied to git commit hashes for fast restarts
3. **Set up incremental updates** for changed files

#### Request Processing

1. **Stage 1**: Apply multiple retrieval signals in parallel (~1-2 seconds)
2. **Stage 2**: Rerank candidates with precise scoring (~1-3 seconds)
3. **Filter**: Apply relevance threshold for final file selection
4. **Context prep**: Load file contents for reasoning model

### Integration with Agent Frameworks

#### With Instant Apply

Two-stage retrieval pairs naturally with Instant Apply:

- **Retrieval**: Find relevant files efficiently
- **Reasoning**: Generate edit snippets using frontier models
- **Application**: Apply changes with high-speed merging

#### With Multi-Agent Systems

Use retrieval as a shared service:

- **Central retrieval agent**: Handles all file discovery
- **Specialized agents**: Focus on specific tasks (debugging, refactoring, etc.)
- **Coordination layer**: Routes requests to appropriate agents

## Example Use Cases

### Bug Fix Scenarios

**Query**: "Fix memory leak in user session management"

**Stage 1 Results**:

- Session-related files (keyword matching)
- Recently changed authentication code (recency signal)
- Memory management utilities (semantic similarity)

**Stage 2 Refinement**:

- Filters to session lifecycle management
- Identifies cleanup methods and destructors
- Finds related test files for validation

### Feature Development

**Query**: "Add real-time notifications with WebSocket support"

**Stage 1 Results**:

- Existing WebSocket implementations (pattern matching)
- Notification-related components (semantic similarity)
- Real-time communication modules (conceptual matching)

**Stage 2 Refinement**:

- Prioritizes WebSocket server setup files
- Identifies notification rendering components
- Finds integration points in existing real-time features

## Performance Optimization Strategies

### Caching for Speed

#### Embedding Cache Management

- **Cache key**: Use git commit hash for invalidation
- **Storage**: Persist embeddings to disk between sessions
- **Incremental updates**: Only re-embed changed files
- **Memory management**: Load only needed embeddings for large repos

#### Performance Impact

| Repository Size          | Initial Index | Cached Load | Incremental Update |
| :----------------------- | :------------ | :---------- | :----------------- |
| **Small (500 files)**    | 30 seconds    | 2 seconds   | 5 seconds          |
| **Medium (2,000 files)** | 2 minutes     | 8 seconds   | 15 seconds         |
| **Large (10,000 files)** | 8 minutes     | 30 seconds  | 45 seconds         |

### Parallel Processing

#### Concurrent Stage 1 Operations

Run multiple retrieval signals simultaneously:

- Git operations in parallel threads
- Batch embedding API calls
- Asynchronous file system operations

#### API Optimization

- **Batch embeddings**: Process multiple files per API call
- **Connection pooling**: Reuse HTTP connections
- **Request pipelining**: Overlap API calls with processing

### Memory Efficiency

#### Smart Loading Strategies

- **Lazy loading**: Load embeddings only when needed
- **LRU cache**: Keep frequently accessed embeddings in memory
- **Streaming**: Process large files in chunks
- **Garbage collection**: Clean up unused embeddings

#### Resource Management

| Strategy      | Memory Usage | Retrieval Speed | Best For        |
| :------------ | :----------- | :-------------- | :-------------- |
| **Load all**  | High         | Fastest         | Small codebases |
| **Lazy load** | Low          | Medium          | Large codebases |
| **LRU cache** | Medium       | Fast            | Mixed workloads |

## Strategic Best Practices

### File Selection Strategy

#### Prioritize High-Impact File Types

Focus indexing efforts on files most likely to contain relevant logic:

- **Source code**: `.py`, `.js`, `.ts`, `.tsx`, `.jsx`, `.java`, `.cpp`
- **Configuration**: `.json`, `.yaml`, `.toml`, `.ini`, `.env`
- **Documentation**: `.md`, `.rst`, `.txt` (for context understanding)
- **Tests**: Test files often reveal usage patterns and edge cases

#### Handle Large Files Intelligently

- **Size limits**: Skip files >100KB that might dilute embedding quality
- **Generated code**: Exclude build artifacts, minified files, auto-generated code
- **Binary files**: Focus only on text-based source files

### Performance Monitoring

Track key metrics to optimize your retrieval system:

#### Retrieval Quality Metrics

- **Precision**: Percentage of retrieved files that are actually relevant
- **Recall**: Percentage of relevant files that were retrieved
- **Time to relevance**: How quickly the most important files are found

#### Performance Metrics

- **Stage 1 latency**: Broad retrieval time (target: < 2 seconds)
- **Stage 2 latency**: Reranking time (target: < 3 seconds)
- **Token efficiency**: Input tokens used vs. baseline approach
- **Cache hit rate**: Percentage of requests served from cache

### Common Pitfalls and Solutions

#### Over-Retrieval (Too Many Files)

**Problem**: Stage 1 returns too many candidates, slowing Stage 2
**Solutions**:

- Increase embedding similarity thresholds
- Add more aggressive keyword filtering
- Implement relevance score minimums

#### Under-Retrieval (Missing Key Files)

**Problem**: Important files not found in Stage 1 candidates
**Solutions**:

- Lower similarity thresholds temporarily
- Add dependency analysis (imports, references)
- Include file path pattern matching
- Weight recent changes more heavily

#### Slow Performance

**Problem**: Retrieval takes >10 seconds consistently
**Solutions**:

- Implement embedding caching strategies
- Use parallel processing for Stage 1 signals
- Consider smaller embedding batches
- Optimize file filtering logic

#### Poor Relevance Quality

**Problem**: Stage 2 returns irrelevant files with high scores
**Solutions**:

- Adjust Code Reranker threshold (try 0.15+ for more precision)
- Improve Stage 1 candidate quality
- Add query preprocessing/clarification
- Consider query-specific retrieval strategies

## Integration Patterns

### With Instant Apply

Two-stage retrieval creates the perfect setup for Instant Apply workflows:

1. **Retrieve**: Find 5-15 most relevant files efficiently
2. **Context**: Load file contents for reasoning model
3. **Generate**: Create targeted edit snippets
4. **Apply**: Use Instant Apply for high-speed, accurate merging

### With Continuous Integration

Embed retrieval into your development workflow:

- **Pre-commit hooks**: Update embeddings for changed files
- **Branch analysis**: Compare retrieval results across feature branches
- **Code review**: Surface related files for reviewer context
- **Documentation**: Auto-link code changes to relevant docs

### Multi-Agent Coordination

Use two-stage retrieval as a shared service:

- **Retrieval agent**: Centralized file discovery service
- **Task agents**: Specialized for debugging, refactoring, testing
- **Coordination**: Route queries to appropriate specialist agents
- **Context sharing**: Pass retrieved files between agents efficiently

## When to Use Two-Stage Retrieval

### Ideal Scenarios

- **Large codebases** (1,000+ files)
- **Complex queries** requiring conceptual understanding
- **High-frequency usage** where caching provides value
- **Token-sensitive applications** where cost matters
- **Quality-critical tasks** where missing relevant code is expensive

### Alternative Approaches

- **Small codebases** (< 500 files): Consider feeding entire codebase to context
- **Simple queries**: Direct keyword search might suffice
- **One-off tasks**: Setup overhead might not be worth it
- **Real-time applications**: Consider simplified single-stage approaches

Two-stage retrieval transforms AI agents from context-limited tools into scalable, efficient systems that can work with codebases of any size while maintaining high accuracy and reasonable costs.

## Related Resources

For implementation details and API specifications:

- [Code Reranker Overview](/docs/code-reranker/overview) - Accuracy benchmarks and threshold strategies
- [Code Reranker Quickstart](/docs/code-reranker/quickstart) - Getting started with the ranking API
- [Embeddings Quickstart](/docs/embeddings/quickstart) - Vector similarity search implementation
- [Agent Setup Guide](/docs/agent-guide) - Comprehensive agent architecture patterns
- [Instant Apply Agent Integration](/docs/instant-apply/agent) - Combining retrieval with code application
