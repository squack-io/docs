---
title: 'Apply Code'
openapi: 'POST /v1/code/apply'
---

Relace apply models run at up to 10,000 tok/s, making complicated code merges feel instantaneous.

## Models

We support two families of models, `lite` and `main`. The `lite` family has fewer parameters, and is highly accurate for shorter requests (less than 16k tokens). The `main` family is designed specifically to improve accuracy for long context tasks (>16k tokens).

| Model                   | Speed       | Max Input | Use Case                         |
| :---------------------- | :---------- | :-------- | :------------------------------- |
| `auto`                  |             | 40k tok   | Auto-route based on input size   |
| `relace-apply-2.5-lite` | ~10k tok/s  | 16k tok   | Fast & accurate on short context |
| `relace-apply-2`        | ~7.5k tok/s | 40k tok   | More accurate on long context    |

We strongly recommend using the `auto` option for best performance.

## OpenAI Compatible Endpoint

If the Relace REST API is inconvenient, we also support an OpenAI compatible endpoint for our apply models.

```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://instantapply.endpoint.relace.run/v1/apply"
)

user_message = """
<instruction>{instruction}</instruction>
<code>{initial_code}</code>
<update>{edit_snippet}</update>
"""

response = client.chat.completions.create(
    model="auto",
    messages=[
        {
            "role": "user",
            "content": user_message
        }
    ]
)
```

<Warning>
  The user message must include `<code>` and `<update>` tags following the format above. The `<instruction>` tag is optional.
</Warning>

## Fallbacks

We recommend using GPT-4.1-mini with [predictive edits](https://platform.openai.com/docs/guides/predicted-outputs) as a fallback. This option is 10-20x slower than Relace's apply models, but it's useful for redundancy.

Relace apply models also return a `400` error code when input exceeds context limits (see table above). For these cases, GPT-4o-mini's 1M token context window is a reliable fallback option. 

However, even frontier LLMs struggle with long context. We recommend proactive refactoring of files >32k tokens to improve output quality. 
