---
title: "Overview"
description: "High-speed code generation with speculative decoding"
icon: "bolt"
---

## Why Fast Apply?

Fast Apply addresses key challenges in code generation:

1. **Eliminates Output Degradation**: Traditional large language models tend to abbreviate or omit functionality on long-context tasks. Common approaches like Search/Replace or Diff formats become token-inefficient with 300B+ models, leading to slower and less reliable results.

2. **Superior Speed**: With speeds of 900-1200 tokens/second, Fast Apply provides significant performance benefits for realtime generation, especially crucial for multi-LLM chains requiring parallel code generation.

3. **Contextual Understanding**: Fast Apply maintains high accuracy by deeply understanding code context and dependencies, enabling it to generate code that seamlessly integrates with existing codebases while respecting established patterns and conventions.

## Where Fast Apply Fits in AI Agent Systems

Fast Apply is optimally positioned for snippet-based code generation within larger AI agent workflows:

1. **Multi-File Code Generation**: When agents need to modify multiple files or make complex changes across a codebase, traditional approaches face significant challenges:

   - Large models like Claude take 60-120 seconds to output complete files
   - Context windows limit the amount of code that can be considered

2. **Snippet-First Architecture**: Instead of asking SOTA models to generate entire files, a more efficient approach emerges:

   - Use primary model (e.g., Claude) to generate focused code snippets
   - Snippets are naturally in-distribution for these models

3. **Optimized Division of Labor**:
   - Primary LLM: Understands high-level intent and generates targeted snippets
   - Fast Apply: Rapidly and accurately merges snippets into existing code
   - Result: 5-10x faster execution with higher reliability

This architecture significantly reduces latency while maintaining high accuracy, making it ideal for real-time agent systems that need to generate and modify code across multiple files.

## Our Models

We currently offer two specialized models:

| Model  | Speed      | Use Case                                   |
| ------ | ---------- | ------------------------------------------ |
| Fast   | 900 tok/s  | Balance of speed and processing complexity |
| Faster | 1200 tok/s | Maximum speed for realtime applications    |
